{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Import Libraries and Load Data](#1)\n",
    "- [2 - Text Cleaning](#2)\n",
    "    - [2.1 - Handle Diacritics using Text Normalization](#2-1)\n",
    "    - [2.1 - Remove user handles](#2-2)\n",
    "    - [2.2 - Remove the URLs](#2-3)\n",
    "    - [2.3 - Tokenize using TweetTokenizer](#2-4)\n",
    "    - [2.4 - Remove Stopwords](#2-5)\n",
    "    - [2.5 - Spelling Corrections](#2-6)\n",
    "    - [2.6 - Remove #symbols while retaining the text](#2-7)\n",
    "    - [2.7 - Remove single and double character length tokens ](#2-8)\n",
    "    - [2.8 - Remove digits](#2-9)\n",
    "    - [2.9 - Remove non alpha numeric characters ](#2-10)\n",
    "\n",
    "    \n",
    "- [3 - Exploratory Data Analysis](#3)\n",
    "    - [3.1 - Check for data imbalance](#3-1)\n",
    "    - [3.2 - Check top terms in the tweet](#3-2)\n",
    "    \n",
    "- [ 4 - Predictive Modeling](#4)\n",
    "    - [4.1 - Data Formatting for Predidictive Modeling](#4-1)\n",
    "    - [4.2 - Using tf-idf vectorizer to generate the feature vectors](#4-2)\n",
    "    - [4-3 - Model using Ordinary Logistic Regression with Default Parameters](#4-3)\n",
    "    - [4-4 - Model Evaluation](#4-4)\n",
    "    - [4-5 - Model using Weighted Logistic Regression to handle data imbalance](#4-5)\n",
    "    - [4-6 - Model Fine Tuning using Randomized Grid Search](#4-6)\n",
    "    - [4-7 - Fine Tuned Model Prediction & Evaluation with balanced class weights](#4-7)\n",
    "    - [4-8 - Fine Tuned Model Prediction & Evaluation with imbalanced class weights](#4-8)\n",
    "- [5 - Summary](#5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## _Import Libraries and Load Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general packages for data manipulation\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#consistent sized plot \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize']=12,5\n",
    "rcParams['axes.labelsize']=12\n",
    "rcParams['xtick.labelsize']=12\n",
    "rcParams['ytick.labelsize']=12\n",
    "\n",
    "#handle the warnings in the code\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "#text preprocessing libraries\n",
    "import nltk #natural language toolkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "#import texthero\n",
    "#import texthero as hero\n",
    "\n",
    "#regular expressions\n",
    "import re\n",
    "\n",
    "#display pandas dataframe columns \n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv file as a pandas dataframe\n",
    "#ISO-8859-1\n",
    "tweet = pd.read_csv('./TwitterHate.csv',delimiter=',',engine='python',encoding='utf-8-sig')\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the identifier number of the tweet\n",
    "tweet.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view one of the tweets randomly \n",
    "random = np.random.randint(0,len(tweet))\n",
    "print(random)\n",
    "tweet.iloc[random]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of the original data to work with \n",
    "df = tweet.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## _Text Cleaning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### _Handle Diacritics using text normalization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(text):\n",
    "    '''Function to handle the diacritics in the text'''\n",
    "    import unicodedata # provides access to the Unicode Character Database (UCD)\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(simplify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### _Remove user handles_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on a sample string\n",
    "sample = \"and @user1 i would like you to discuss with @user2 and then with @username3\"\n",
    "pattern = re.compile(r'@\\w+')\n",
    "re.findall(pattern,sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all the user handles --> strings starting with @\n",
    "df['tweet'].replace(r'@\\w+','',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "### _Remove the urls_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on a sample \n",
    "sample = \"https://www.machinelearing.com prakhar and https://www.simple.com\"\n",
    "pattern = re.compile(r'http\\S+')\n",
    "re.findall(pattern,sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'].replace(r'http\\S+','',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-4'></a>\n",
    "### _Tokenize using tweet tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on a sample text\n",
    "sample = 'wonderfl :-)  when are you coming for #party'\n",
    "tweet_tokenize = TweetTokenizer(preserve_case=True)\n",
    "tweet_tokenize.tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the tweets in the dataframe using TweetTokenizer\n",
    "tokenizer = TweetTokenizer(preserve_case=True)\n",
    "df['tweet'] = df['tweet'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the tokenized tweets\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-5'></a>\n",
    "### _Remove Stopwords_\n",
    "_Append more words to be removed from the text - example rt and amp which occur very frequently_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#add additional stop words to be removed from the text\n",
    "additional_list = ['amp','rt','u',\"can't\",'ur']\n",
    "\n",
    "for words in additional_list:\n",
    "    stop_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "def remove_stopwords(text):\n",
    "    '''Function to remove the stop words from the text corpus'''\n",
    "    clean_text = [word for word in text if not word in stop_words]\n",
    "    return clean_text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the stop words from the tweets\n",
    "df['tweet'] = df['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-6'></a>\n",
    "### _Spelling corrections_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply spelling correction on a sample text\n",
    "from textblob import TextBlob\n",
    "sample = 'amazng man you did it finallyy'\n",
    "txtblob = TextBlob(sample)\n",
    "corrected_text = txtblob.correct()\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textblob expect a string to be passed and not a list of strings\n",
    "from textblob import TextBlob\n",
    "\n",
    "def spell_check(text):\n",
    "    '''Function to do spelling correction using '''\n",
    "    txtblob = TextBlob(text)\n",
    "    corrected_text = txtblob.correct()\n",
    "    return corrected_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-7'></a>\n",
    "### _Remove # symbols while retaining the text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try tremoving # symbols from a sample text\n",
    "sample = '#winner #machine i am learning'\n",
    "pattern = re.compile(r'#')\n",
    "re.sub(pattern,'',sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashsymbols(text):\n",
    "    '''Function to remove the hashtag symbol from the text'''\n",
    "    pattern = re.compile(r'#')\n",
    "    text = ' '.join(text)\n",
    "    clean_text = re.sub(pattern,'',text)\n",
    "    return tokenizer.tokenize(clean_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(remove_hashsymbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-8'></a>\n",
    "### _Remove single and double length characters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_shortwords(text):\n",
    "    '''Function to remove the short words of length 1 and 2 characters'''\n",
    "    '''Arguments: \n",
    "       text: string\n",
    "       returns: string without containing words of length 1 and 2'''\n",
    "    lengths = [1,2]\n",
    "    new_text = ' '.join(text)\n",
    "    for word in text:\n",
    "        text = [word for word in tokenizer.tokenize(new_text) if not len(word) in lengths]\n",
    "        \n",
    "    return new_text       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(rem_shortwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-9'></a>\n",
    "### _Remove digits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_digits(text):\n",
    "    '''Function to remove the digits from the list of strings'''\n",
    "    no_digits = []\n",
    "    for word in text:\n",
    "        no_digits.append(re.sub(r'\\d','',word))\n",
    "    return ' '.join(no_digits)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(rem_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-10'></a>\n",
    "### _Remove special characters_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_nonalpha(text):\n",
    "    '''Function to remove the non-alphanumeric characters from the text'''\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the non alpha numeric characters from the tweet tokens\n",
    "df['tweet'] = df['tweet'].apply(rem_nonalpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## _Exploratory Data Analysis - Broad Approach_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-1'></a>\n",
    "### _Check for data balance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of the count of hate and non hate tweet\n",
    "sns.countplot(x=df[\"label\"])\n",
    "plt.title('Count of Hate vs Non Hate Tweet')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_There are more non hatetextes than the hatetext in the dataset_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-13T08:28:31.024391Z",
     "iopub.status.busy": "2021-08-13T08:28:31.023999Z",
     "iopub.status.idle": "2021-08-13T08:28:31.13622Z",
     "shell.execute_reply": "2021-08-13T08:28:31.135427Z",
     "shell.execute_reply.started": "2021-08-13T08:28:31.024358Z"
    }
   },
   "source": [
    "<a id='3-2'></a>\n",
    "### _Check out the top terms in the tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "results = Counter()\n",
    "df['tweet'].apply(results.update)\n",
    "#print the top 10 most common terms in the tweet \n",
    "print(results.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the cumulative frequency of the top 10 most common tokens \n",
    "frequency = nltk.FreqDist(results)\n",
    "plt.title('Top 10 Most Common Terms')\n",
    "frequency.plot(10,cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the frequency of the top 10 most common tokens \n",
    "frequency = nltk.FreqDist(results)\n",
    "plt.title('Top 10 Most Common Terms')\n",
    "frequency.plot(10,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Love is the most frequently used word followed by day, happy etc. This is expected as there are more non hate tweets than hate tweets in the dataset_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## _Predictive Modeling_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Data Formatting for Predictive Modeling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for the null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the tokens back to form the string\n",
    "df['tweet'] = df['tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the top rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into input X and output y\n",
    "X = df['tweet']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 51\n",
    "test_size = 0.2 #20% of the data in the \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=seed,stratify=df['label'])\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-2'></a>\n",
    "### _Use tf-idf as a feature to get into the vector space model_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the vectorizer \n",
    "vectorizer = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "#transform the test data\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-3'></a>\n",
    "### _Model building: Ordinary Logistic Regression_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the models with default hyper-parameters\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-4'></a>\n",
    "### _Model evaluation_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the model accuracy on the training and the test set\n",
    "print('Accuracy Score on training set %.5f' %accuracy_score(y_train,train_predictions))\n",
    "print('Accuracy Score on test set %.5f' %accuracy_score(y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Accuracy is never a good metric for an imbalanced dataset as in this case. This can be highighted using the f1 score. A low f1-score for a label indicate poor performance of the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report Training set')\n",
    "print('\\n')\n",
    "print(classification_report(y_train,train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report Testing set')\n",
    "print('\\n')\n",
    "print(classification_report(y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The model's f1-score is low for label 1 which indicates the hate text in the twitter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-16T05:09:08.477534Z",
     "iopub.status.busy": "2021-08-16T05:09:08.4771Z",
     "iopub.status.idle": "2021-08-16T05:09:08.481666Z",
     "shell.execute_reply": "2021-08-16T05:09:08.480758Z",
     "shell.execute_reply.started": "2021-08-16T05:09:08.477502Z"
    }
   },
   "source": [
    "<a id='4-5'></a>\n",
    "### _Weighted Logistic Regression Or Cost Sensitive Logistic Regression_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The minority to majority class ratio is 1:13_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the weight of the class labels using inverse ratio\n",
    "weights = {0:1.0,1:13.0}\n",
    "\n",
    "#instantiate the logistic regression model and account for the weights to be applied for model coefficients update magnitude\n",
    "clf = LogisticRegression(solver='lbfgs',class_weight=weights)\n",
    "\n",
    "#fit and predict\n",
    "clf.fit(X_train,y_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "#classification report\n",
    "print('Classification Report Training set')\n",
    "print('------------------------------------')\n",
    "print('\\n')\n",
    "print(classification_report(y_train,train_predictions))\n",
    "print('\\n')\n",
    "\n",
    "print('Classification Report Testing set')\n",
    "print('------------------------------------')\n",
    "print('\\n')\n",
    "print(classification_report(y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The f1 score of both the training and testing set has improved compared to the plain vanilla Logistic Regression model. There is still more opportunity to improve the score using better models or even handling the data imbalance by adding synthetic data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-6'></a>\n",
    "### _Regularization and Hyperparameter tuning:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries for grid search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "from scipy.stats import loguniform\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l1', 'l2', 'elasticnet']\n",
    "space['C'] = loguniform(1e-5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the search space \n",
    "print(space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-7'></a>\n",
    "### _Fine tuned Model with Balanced Class Weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "from scipy.stats import loguniform\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l2', 'l1',  'elasticnet'] # Updated penalty list\n",
    "space['C'] = loguniform(1e-5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],  # all penalty options\n",
    "    'C': loguniform(1e-5, 100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model with balanced class weights\n",
    "weights = {0:1.0,1:1.0}\n",
    "clf = LogisticRegression(class_weight=weights)\n",
    "#define the number of folds \n",
    "# folds = StratifiedKFold(n_splits=4,random_state=seed)\n",
    "folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "# define search\n",
    "grid_search = RandomizedSearchCV(estimator=clf,param_distributions=space, n_iter=100, scoring='recall',\n",
    "                            n_jobs=-1, cv=folds, random_state=seed)\n",
    "#fit grid search on the train data\n",
    "grid_result = grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the best model \n",
    "grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the best model\n",
    "clf = LogisticRegression(C=23.871926754399514,penalty='l1',solver='liblinear',class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and predict\n",
    "clf.fit(X_train,y_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "#classification report\n",
    "print('Classification Report Training set')\n",
    "print('------------------------------------')\n",
    "print('\\n')\n",
    "print(classification_report(y_train,train_predictions))\n",
    "print('\\n')\n",
    "\n",
    "print('Classification Report Testing set')\n",
    "print('------------------------------------')\n",
    "print('\\n')\n",
    "print(classification_report(y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-8'></a>\n",
    "### _Fine tuned model with class weights proportional to the class imbalance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the class weights to handle the imbalance in the labels\n",
    "weights = {0:1.0,1:13}\n",
    "\n",
    "clf = LogisticRegression(class_weight=weights)\n",
    "#define the number of folds \n",
    "# folds = StratifiedKFold(n_splits=4,random_state=seed)\n",
    "folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "# define search\n",
    "grid_search = RandomizedSearchCV(estimator=clf,param_distributions=space, n_iter=100, scoring='recall',\n",
    "                            n_jobs=-1, cv=folds, random_state=seed)\n",
    "#fit grid search on the train data\n",
    "grid_result = grid_search.fit(X_train,y_train)\n",
    "\n",
    "#retrieve the best model \n",
    "grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the best model\n",
    "clf = LogisticRegression(C=0.16731783677034165,penalty='l2',solver='liblinear',class_weight=weights)\n",
    "\n",
    "#fit and predict\n",
    "clf.fit(X_train,y_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "#classification report\n",
    "print('Classification Report Training set')\n",
    "print('------------------------------------')\n",
    "print('\\n')\n",
    "print(classification_report(y_train,train_predictions))\n",
    "print('\\n')\n",
    "\n",
    "print('Classification Report Testing set')\n",
    "print('------------------------------------')\n",
    "print('\\n')\n",
    "print(classification_report(y_test,test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the confusion matrix display object\n",
    "cm = confusion_matrix(y_test, test_predictions, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp.plot(cmap='summer') \n",
    "plt.title('Confusion Matrix Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## _Summary_\n",
    "\n",
    "- Logistic Regression with default paramaters recall = 29%\n",
    "- Logistic Regression with class weights in proportion to the data imbalance recall = 75%\n",
    "- Logistic Regression fine tuned with grid search and balanced class weights recall = 56%\n",
    "- Logistic Regression fine tuned with grid search and class weights in proportion to data imbalance recall = 77%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries for CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "# Define CNN model\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=len(vectorizer.get_feature_names_out()), output_dim=64, input_length=X_train.shape[1]),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the CNN model\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model\n",
    "cnn_model.fit(X_train.toarray(), y_train, epochs=1, batch_size=64, validation_data=(X_test.toarray(), y_test))\n",
    "\n",
    "# Evaluate CNN model\n",
    "cnn_loss, cnn_acc = cnn_model.evaluate(X_test.toarray(), y_test)\n",
    "print(f\"CNN Model Accuracy: {cnn_acc:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1528042,
     "sourceId": 2522155,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30120,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
